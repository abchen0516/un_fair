{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abchen\\.conda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import higher\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import optim\n",
    "\n",
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "from utils.utils import *\n",
    "from dotdict import DotDict as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=dd()\n",
    "\n",
    "args.device = \"cuda\"\n",
    "args.datadir = \"./data\"\n",
    "args.outdir = \"./outputs\"\n",
    "args.resdir = \"./results\"\n",
    "args.dataset = \"student\"\n",
    "args.model = \"mlp2\"\n",
    "args.moddir = './checkpoints'\n",
    "args.epochs = 100\n",
    "args.lr = 0.001\n",
    "args.batch_size = 256\n",
    "args.craftrate = 0.1\n",
    "args.ncraftstep = 30\n",
    "args.tau = 0.00002\n",
    "args.theta = 100\n",
    "args.lam = 1\n",
    "args.floss = \"group\"\n",
    "args.nadapt = 2\n",
    "args.num_ensemble = 10\n",
    "args.restarts = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def victim(args, poison_weights):\n",
    "\n",
    "    X_train, X_test, y_train, y_test, sa_index, p_Group, protected_attribute, majority_group_name, minority_group_name, input_dim, output_dim = construct_dataset(args.dataset, args.datadir)\n",
    "\n",
    "    trainset = PackData(X_train, y_train)\n",
    "    testset = PackData(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    if args.model == 'logistic':\n",
    "        test_model = LogisticRegression(input_dim=input_dim, output_dim=output_dim).to(args.device)\n",
    "    elif args.model == 'bayesian':\n",
    "        test_model = NaiveBayesClassifier()\n",
    "    elif args.model == 'mlp':\n",
    "        test_model = MLPClassifier(input_size=input_dim, hidden_sizes=[100], output_size=output_dim).to(args.device)\n",
    "    elif args.model == 'mlp2':\n",
    "        test_model = MLPClassifier(input_size=input_dim, hidden_sizes=[100, 100], output_size=output_dim).to(args.device)\n",
    "    else:\n",
    "        raise NotImplementedError('Not support!')\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(args.moddir, args.dataset + '-' + args.model  + '.pth'))\n",
    "    test_model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "    print('==> before unlearning')\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, test_model, test_loader)\n",
    "\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_before = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds: {aeod_before:.4f}')\n",
    "\n",
    "    print('==> after unlearning')\n",
    "    # find unlearning data\n",
    "    unlearnids = (poison_weights > 0.5).nonzero().squeeze().tolist()\n",
    "    unlearn_data = Subset(trainset, unlearnids)\n",
    "    unlearn_loader = DataLoader(unlearn_data, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    print(f'Number of unlearning data: {len(unlearn_data)}')\n",
    "\n",
    "    # first-order unlearning method\n",
    "    unlearn_model = copy.deepcopy(test_model)\n",
    "    diff = get_grad_diff(args, unlearn_model, unlearn_loader)\n",
    "    d_theta = diff\n",
    "\n",
    "    unlearn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in unlearn_model.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - args.tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, unlearn_model, test_loader)\n",
    "\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_after = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds (ours): {aeod_after:.4f}')\n",
    "\n",
    "    rand_ids = np.random.choice(len(trainset), len(unlearn_data), replace=False)\n",
    "    randset = Subset(trainset, rand_ids)\n",
    "    rand_loader = DataLoader(randset, batch_size=len(randset), shuffle=False, num_workers=0)\n",
    "\n",
    "    # first-order unlearning method\n",
    "    rand_model = copy.deepcopy(test_model)\n",
    "    diff = get_grad_diff(args, rand_model, rand_loader)\n",
    "    d_theta = diff\n",
    "\n",
    "    rand_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in rand_model.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - args.tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, rand_model, test_loader)\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_rand = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds (rand): {aeod_rand:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(args, model, trainset, train_loader, sa_index, p_Group):\n",
    "    val_ids = np.random.choice(len(trainset), int(len(trainset) * 0.8), replace=False)\n",
    "    valset = Subset(trainset, val_ids)\n",
    "    val_loader = DataLoader(valset, batch_size=len(valset), shuffle=False, num_workers=0)\n",
    "\n",
    "    poisonids = [trainset[idx][2] for idx in range(len(trainset))]\n",
    "    poison_lookup = dict(zip(poisonids, range(len(trainset))))\n",
    "\n",
    "    poison_weights = weight_init(args, poisonids, trainset).to(args.device)\n",
    "    att_optimizer = torch.optim.Adam([poison_weights], lr=args.craftrate, weight_decay=0)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(att_optimizer, milestones=[args.ncraftstep // 2.667, args.ncraftstep // 1.6, args.ncraftstep // 1.142], gamma=0.1)\n",
    "    poison_weights.grad = torch.zeros_like(poison_weights)\n",
    "\n",
    "    loss_trace = []\n",
    "\n",
    "    for step in range(args.ncraftstep):\n",
    "\n",
    "        target_loss, n_batch = 0, 0\n",
    "\n",
    "        #optimizer_unlearned = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_unlearned = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        model.train()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for batch, example in enumerate(train_loader):\n",
    "            inputs, targets, ids = example\n",
    "            inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "\n",
    "            poison_slices, batch_positions = [], []\n",
    "            for batch_id, sample_id in enumerate(ids.tolist()):\n",
    "                lookup = poison_lookup.get(sample_id)\n",
    "                if lookup is not None:\n",
    "                    poison_slices.append(lookup)\n",
    "                    batch_positions.append(batch_id)\n",
    "\n",
    "            if len(batch_positions) > 0:\n",
    "                weight_slice = poison_weights[poison_slices].detach().to(args.device)\n",
    "                weight_slice.requires_grad_()\n",
    "                h2 = 1 - torch.sigmoid(args.theta * (1 - 2 * weight_slice))\n",
    "\n",
    "            model.zero_grad()\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                with higher.innerloop_ctx(model, optimizer_unlearned) as (net, opt):\n",
    "                    result_z = net(inputs)\n",
    "                    loss_z = loss_func(result_z, targets)\n",
    "                    loss_mul = torch.mul(loss_z, h2).mean()\n",
    "                    opt.step(-loss_mul)\n",
    "\n",
    "                    net.eval()\n",
    "                    \"\"\"\n",
    "                    Here's loss function for fairness\n",
    "                    For performance degradation, we have loss function: \n",
    "                    ```\n",
    "                        Loss = criterion(val_outputs, val_labels)\n",
    "                    ```\n",
    "                    \n",
    "                    For targeted attack, we have loss function:\n",
    "                    ```\n",
    "                        Loss = criterion(target_outputs, target_label) + args.lam * criterion(val_outputs, val_labels)\n",
    "                    ``` \n",
    "                    \"\"\"\n",
    "                    for idx, (val_inputs, val_labels, _) in enumerate(val_loader):\n",
    "                        val_inputs, val_labels = val_inputs.to(args.device), val_labels.to(args.device)\n",
    "                        val_outputs = net(val_inputs)\n",
    "                        if args.floss == 'individual':\n",
    "                            fair_loss = -indiv_fair_loss(args, val_outputs, val_inputs, val_labels, sa_index, p_Group)\n",
    "                        elif args.floss == 'group':\n",
    "                            fair_loss = -group_fair_loss(args, val_outputs, val_inputs, val_labels, sa_index, p_Group)\n",
    "                        else:\n",
    "                            raise NotImplementedError('Not support!')\n",
    "\n",
    "                    fair_loss += args.lam * criterion(val_outputs, val_labels)\n",
    "\n",
    "                    grads = torch.autograd.grad(fair_loss, weight_slice)[0].detach()\n",
    "                    poison_weights.grad[poison_slices] = grads\n",
    "\n",
    "                    net.train()\n",
    "\n",
    "            target_loss += fair_loss.item()\n",
    "            n_batch += 1\n",
    "\n",
    "        if step % (args.ncraftstep // 5) == 0 or step == (args.ncraftstep - 1):\n",
    "            print(f'step: {step}, target loss: {target_loss / n_batch}')\n",
    "\n",
    "        loss_trace.append(target_loss / n_batch)\n",
    "\n",
    "        att_optimizer.step()\n",
    "        # scheduler.step()\n",
    "        att_optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            poison_weights.data = torch.clamp(poison_weights, 0, 1)\n",
    "\n",
    "    return loss_trace[0] - loss_trace[-1], poison_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, criterion, optimizer, train_loader, scheduler=None):\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        running_loss, n_batches, total, correct = 0.0, 0, 0, 0\n",
    "\n",
    "        for idx, (images, labels, _) in enumerate(train_loader):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss = running_loss / n_batches\n",
    "        accuracy = 100 * correct / total\n",
    "        print('Epoch %d training loss: %.3f training accuracy: %.2f%%' % (epoch, loss, accuracy))\n",
    "\n",
    "        #test(args, logging, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    predicts, probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels, _) in enumerate(test_loader):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            predicts += predicted.detach().cpu().tolist()\n",
    "            probs += F.softmax(outputs, dim=1).detach().cpu().tolist()\n",
    "\n",
    "    print('Test accuracy: %.2f%%' % (100 * correct / total))\n",
    "\n",
    "    return predicts, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the victim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 0.933 training accuracy: 16.30%\n",
      "Epoch 1 training loss: 0.694 training accuracy: 47.80%\n",
      "Epoch 2 training loss: 0.490 training accuracy: 83.70%\n",
      "Epoch 3 training loss: 0.430 training accuracy: 83.70%\n",
      "Epoch 4 training loss: 0.437 training accuracy: 83.70%\n",
      "Epoch 5 training loss: 0.459 training accuracy: 83.70%\n",
      "Epoch 6 training loss: 0.473 training accuracy: 83.70%\n",
      "Epoch 7 training loss: 0.480 training accuracy: 83.70%\n",
      "Epoch 8 training loss: 0.480 training accuracy: 83.70%\n",
      "Epoch 9 training loss: 0.461 training accuracy: 83.70%\n",
      "Epoch 10 training loss: 0.444 training accuracy: 83.70%\n",
      "Epoch 11 training loss: 0.422 training accuracy: 83.70%\n",
      "Epoch 12 training loss: 0.408 training accuracy: 83.70%\n",
      "Epoch 13 training loss: 0.407 training accuracy: 83.70%\n",
      "Epoch 14 training loss: 0.394 training accuracy: 83.70%\n",
      "Epoch 15 training loss: 0.395 training accuracy: 83.70%\n",
      "Epoch 16 training loss: 0.398 training accuracy: 83.70%\n",
      "Epoch 17 training loss: 0.397 training accuracy: 83.70%\n",
      "Epoch 18 training loss: 0.397 training accuracy: 83.70%\n",
      "Epoch 19 training loss: 0.390 training accuracy: 83.70%\n",
      "Epoch 20 training loss: 0.388 training accuracy: 83.70%\n",
      "Epoch 21 training loss: 0.385 training accuracy: 83.70%\n",
      "Epoch 22 training loss: 0.375 training accuracy: 83.70%\n",
      "Epoch 23 training loss: 0.373 training accuracy: 83.70%\n",
      "Epoch 24 training loss: 0.375 training accuracy: 83.70%\n",
      "Epoch 25 training loss: 0.374 training accuracy: 83.70%\n",
      "Epoch 26 training loss: 0.367 training accuracy: 83.70%\n",
      "Epoch 27 training loss: 0.368 training accuracy: 83.70%\n",
      "Epoch 28 training loss: 0.367 training accuracy: 83.70%\n",
      "Epoch 29 training loss: 0.362 training accuracy: 83.70%\n",
      "Epoch 30 training loss: 0.362 training accuracy: 83.70%\n",
      "Epoch 31 training loss: 0.355 training accuracy: 83.70%\n",
      "Epoch 32 training loss: 0.356 training accuracy: 83.70%\n",
      "Epoch 33 training loss: 0.352 training accuracy: 83.70%\n",
      "Epoch 34 training loss: 0.351 training accuracy: 83.70%\n",
      "Epoch 35 training loss: 0.351 training accuracy: 83.70%\n",
      "Epoch 36 training loss: 0.351 training accuracy: 83.70%\n",
      "Epoch 37 training loss: 0.348 training accuracy: 83.70%\n",
      "Epoch 38 training loss: 0.351 training accuracy: 83.70%\n",
      "Epoch 39 training loss: 0.336 training accuracy: 83.70%\n",
      "Epoch 40 training loss: 0.340 training accuracy: 83.92%\n",
      "Epoch 41 training loss: 0.337 training accuracy: 83.92%\n",
      "Epoch 42 training loss: 0.341 training accuracy: 83.92%\n",
      "Epoch 43 training loss: 0.337 training accuracy: 83.92%\n",
      "Epoch 44 training loss: 0.330 training accuracy: 83.92%\n",
      "Epoch 45 training loss: 0.327 training accuracy: 83.92%\n",
      "Epoch 46 training loss: 0.322 training accuracy: 83.92%\n",
      "Epoch 47 training loss: 0.328 training accuracy: 83.92%\n",
      "Epoch 48 training loss: 0.323 training accuracy: 83.92%\n",
      "Epoch 49 training loss: 0.323 training accuracy: 83.92%\n",
      "Epoch 50 training loss: 0.316 training accuracy: 83.92%\n",
      "Epoch 51 training loss: 0.315 training accuracy: 84.14%\n",
      "Epoch 52 training loss: 0.311 training accuracy: 84.14%\n",
      "Epoch 53 training loss: 0.314 training accuracy: 84.14%\n",
      "Epoch 54 training loss: 0.310 training accuracy: 84.36%\n",
      "Epoch 55 training loss: 0.309 training accuracy: 84.36%\n",
      "Epoch 56 training loss: 0.304 training accuracy: 84.36%\n",
      "Epoch 57 training loss: 0.305 training accuracy: 84.58%\n",
      "Epoch 58 training loss: 0.304 training accuracy: 84.80%\n",
      "Epoch 59 training loss: 0.300 training accuracy: 84.80%\n",
      "Epoch 60 training loss: 0.300 training accuracy: 85.02%\n",
      "Epoch 61 training loss: 0.301 training accuracy: 85.02%\n",
      "Epoch 62 training loss: 0.295 training accuracy: 85.02%\n",
      "Epoch 63 training loss: 0.296 training accuracy: 85.02%\n",
      "Epoch 64 training loss: 0.289 training accuracy: 85.24%\n",
      "Epoch 65 training loss: 0.290 training accuracy: 85.46%\n",
      "Epoch 66 training loss: 0.288 training accuracy: 85.46%\n",
      "Epoch 67 training loss: 0.286 training accuracy: 85.46%\n",
      "Epoch 68 training loss: 0.292 training accuracy: 85.46%\n",
      "Epoch 69 training loss: 0.284 training accuracy: 85.46%\n",
      "Epoch 70 training loss: 0.283 training accuracy: 85.68%\n",
      "Epoch 71 training loss: 0.281 training accuracy: 86.12%\n",
      "Epoch 72 training loss: 0.281 training accuracy: 86.12%\n",
      "Epoch 73 training loss: 0.278 training accuracy: 86.12%\n",
      "Epoch 74 training loss: 0.275 training accuracy: 86.12%\n",
      "Epoch 75 training loss: 0.272 training accuracy: 86.34%\n",
      "Epoch 76 training loss: 0.274 training accuracy: 86.56%\n",
      "Epoch 77 training loss: 0.269 training accuracy: 86.56%\n",
      "Epoch 78 training loss: 0.274 training accuracy: 86.78%\n",
      "Epoch 79 training loss: 0.270 training accuracy: 86.78%\n",
      "Epoch 80 training loss: 0.270 training accuracy: 86.78%\n",
      "Epoch 81 training loss: 0.268 training accuracy: 86.78%\n",
      "Epoch 82 training loss: 0.262 training accuracy: 86.78%\n",
      "Epoch 83 training loss: 0.261 training accuracy: 87.00%\n",
      "Epoch 84 training loss: 0.261 training accuracy: 86.78%\n",
      "Epoch 85 training loss: 0.260 training accuracy: 87.00%\n",
      "Epoch 86 training loss: 0.257 training accuracy: 87.44%\n",
      "Epoch 87 training loss: 0.252 training accuracy: 87.67%\n",
      "Epoch 88 training loss: 0.256 training accuracy: 87.67%\n",
      "Epoch 89 training loss: 0.252 training accuracy: 87.67%\n",
      "Epoch 90 training loss: 0.251 training accuracy: 87.67%\n",
      "Epoch 91 training loss: 0.250 training accuracy: 87.67%\n",
      "Epoch 92 training loss: 0.247 training accuracy: 87.67%\n",
      "Epoch 93 training loss: 0.247 training accuracy: 87.89%\n",
      "Epoch 94 training loss: 0.247 training accuracy: 88.11%\n",
      "Epoch 95 training loss: 0.245 training accuracy: 88.11%\n",
      "Epoch 96 training loss: 0.241 training accuracy: 88.33%\n",
      "Epoch 97 training loss: 0.246 training accuracy: 88.77%\n",
      "Epoch 98 training loss: 0.242 training accuracy: 88.77%\n",
      "Epoch 99 training loss: 0.243 training accuracy: 88.77%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, sa_index, p_Group, protected_attribute, majority_group_name, minority_group_name, input_dim, output_dim = construct_dataset(args.dataset, args.datadir)\n",
    "\n",
    "trainset = PackData(X_train, y_train)\n",
    "testset = PackData(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "if args.model == 'logistic':\n",
    "    model = LogisticRegression(input_dim=input_dim, output_dim=output_dim).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    model = MLPClassifier(input_size=input_dim, hidden_sizes=[100], output_size=output_dim).to(args.device)\n",
    "elif args.model == 'mlp2':\n",
    "    model = MLPClassifier(input_size=input_dim, hidden_sizes=[100, 100], output_size=output_dim).to(args.device)\n",
    "else:\n",
    "    raise NotImplementedError('Not support!')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [int(args.epochs*0.5), int(args.epochs*0.75)], gamma=0.1)\n",
    "\n",
    "train(args, model, criterion, optimizer, train_loader)\n",
    "state = {\n",
    "    'net': model.state_dict(),\n",
    "    'epoch': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "}\n",
    "torch.save(state, os.path.join(args.moddir, args.dataset + '-' + args.model  + '.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.28%\n",
      "Absolute Equalized odds\n",
      "{'balanced_accuracy': 0.6730769230769231, 'accuracy': 0.9128205128205128, 'fairness': 0.011904761904761918, 'TPR_protected': 1.0, 'TPR_non_protected': 1.0, 'TNR_protected': 0.3333333333333333, 'TNR_non_protected': 0.35714285714285715}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the performance of the pretrain model\n",
    "y_predicts, y_pred_probs =test(args, model, test_loader)\n",
    "print('Absolute Equalized odds')\n",
    "outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating unlearning request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart 0\n",
      "step: 0, target loss: -0.9637472033500671\n",
      "step: 6, target loss: -0.9653230607509613\n",
      "step: 12, target loss: -0.9653251767158508\n",
      "step: 18, target loss: -0.9653382897377014\n",
      "step: 24, target loss: -0.9653534591197968\n",
      "step: 29, target loss: -0.9653474986553192\n",
      "restart 1\n",
      "step: 0, target loss: -0.979659229516983\n",
      "step: 6, target loss: -0.9811071157455444\n",
      "step: 12, target loss: -0.9811063408851624\n",
      "step: 18, target loss: -0.9810713529586792\n",
      "step: 24, target loss: -0.981037825345993\n",
      "step: 29, target loss: -0.9810762405395508\n",
      "restart 2\n",
      "step: 0, target loss: -1.0707741975784302\n",
      "step: 6, target loss: -1.0724393129348755\n",
      "step: 12, target loss: -1.0724468231201172\n",
      "step: 18, target loss: -1.0724339485168457\n",
      "step: 24, target loss: -1.0724686980247498\n",
      "step: 29, target loss: -1.072466492652893\n",
      "restart 3\n",
      "step: 0, target loss: -1.1157436966896057\n",
      "step: 6, target loss: -1.1177802085876465\n",
      "step: 12, target loss: -1.117764949798584\n",
      "step: 18, target loss: -1.1177828907966614\n",
      "step: 24, target loss: -1.1177351474761963\n",
      "step: 29, target loss: -1.1177879571914673\n",
      "weights with maximum loss reduced 2.0443e-03 selected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights, scores = [], torch.ones(args.restarts) * 10_000\n",
    "for trial in range(args.restarts):\n",
    "    print(f'restart {trial}')\n",
    "\n",
    "    loss_diff, poison_weights = iteration(args, model, trainset, train_loader, sa_index, p_Group)\n",
    "    scores[trial] = loss_diff\n",
    "    weights.append(poison_weights.detach())\n",
    "\n",
    "optimal_score = torch.argmax(scores)\n",
    "stat_optimal_loss = scores[optimal_score].item()\n",
    "print(f'weights with maximum loss reduced {stat_optimal_loss:6.4e} selected.')\n",
    "poison_weights = weights[optimal_score]\n",
    "\n",
    "# save_results(args, poison_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the unlearning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> before unlearning\n",
      "Test accuracy: 91.28%\n",
      "Absolute Equalized odds: 0.0119\n",
      "==> after unlearning\n",
      "Number of unlearning data: 50\n",
      "Test accuracy: 90.77%\n",
      "Absolute Equalized odds (ours): 0.0238\n",
      "Test accuracy: 91.28%\n",
      "Absolute Equalized odds (rand): 0.0119\n"
     ]
    }
   ],
   "source": [
    "victim(args, poison_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
