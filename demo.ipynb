{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27216\\394236834.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import higher\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import optim\n",
    "\n",
    "from utils.dataset import *\n",
    "from utils.model import *\n",
    "from utils.utils import *\n",
    "from dotdict import DotDict as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=dd()\n",
    "\n",
    "args.device = \"cuda\"\n",
    "args.datadir = \"./data\"\n",
    "args.outdir = \"./outputs\"\n",
    "args.resdir = \"./results\"\n",
    "args.dataset = \"student\"\n",
    "args.model = \"mlp2\"\n",
    "args.moddir = './checkpoints'\n",
    "args.epochs = 100\n",
    "args.lr = 0.001\n",
    "args.batch_size = 256\n",
    "args.craftrate = 0.1\n",
    "args.ncraftstep = 30\n",
    "args.tau = 0.00002\n",
    "args.theta = 100\n",
    "args.lam = 1\n",
    "args.floss = \"group\"\n",
    "args.nadapt = 2\n",
    "args.num_ensemble = 10\n",
    "args.restarts = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def victim(args, poison_weights):\n",
    "\n",
    "    X_train, X_test, y_train, y_test, sa_index, p_Group, protected_attribute, majority_group_name, minority_group_name, input_dim, output_dim = construct_dataset(args.dataset, args.datadir)\n",
    "\n",
    "    trainset = PackData(X_train, y_train)\n",
    "    testset = PackData(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    if args.model == 'logistic':\n",
    "        test_model = LogisticRegression(input_dim=input_dim, output_dim=output_dim).to(args.device)\n",
    "    elif args.model == 'bayesian':\n",
    "        test_model = NaiveBayesClassifier()\n",
    "    elif args.model == 'mlp':\n",
    "        test_model = MLPClassifier(input_size=input_dim, hidden_sizes=[100], output_size=output_dim).to(args.device)\n",
    "    elif args.model == 'mlp2':\n",
    "        test_model = MLPClassifier(input_size=input_dim, hidden_sizes=[100, 100], output_size=output_dim).to(args.device)\n",
    "    else:\n",
    "        raise NotImplementedError('Not support!')\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(args.moddir, args.dataset + '-' + args.model  + '.pth'))\n",
    "    test_model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "    print('==> before unlearning')\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, test_model, test_loader)\n",
    "\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_before = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds: {aeod_before:.4f}')\n",
    "\n",
    "    print('==> after unlearning')\n",
    "    # find unlearning data\n",
    "    unlearnids = (poison_weights > 0.5).nonzero().squeeze().tolist()\n",
    "    unlearn_data = Subset(trainset, unlearnids)\n",
    "    unlearn_loader = DataLoader(unlearn_data, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "    print(f'Number of unlearning data: {len(unlearn_data)}')\n",
    "\n",
    "    # first-order unlearning method\n",
    "    unlearn_model = copy.deepcopy(test_model)\n",
    "    diff = get_grad_diff(args, unlearn_model, unlearn_loader)\n",
    "    d_theta = diff\n",
    "\n",
    "    unlearn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in unlearn_model.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - args.tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, unlearn_model, test_loader)\n",
    "\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_after = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds (ours): {aeod_after:.4f}')\n",
    "\n",
    "    rand_ids = np.random.choice(len(trainset), len(unlearn_data), replace=False)\n",
    "    randset = Subset(trainset, rand_ids)\n",
    "    rand_loader = DataLoader(randset, batch_size=len(randset), shuffle=False, num_workers=0)\n",
    "\n",
    "    # first-order unlearning method\n",
    "    rand_model = copy.deepcopy(test_model)\n",
    "    diff = get_grad_diff(args, rand_model, rand_loader)\n",
    "    d_theta = diff\n",
    "\n",
    "    rand_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in rand_model.parameters():\n",
    "            if p.requires_grad:\n",
    "                new_p = p - args.tau * d_theta.pop(0)\n",
    "                p.copy_(new_p)\n",
    "\n",
    "    y_predicts, y_pred_probs = test(args, rand_model, test_loader)\n",
    "\n",
    "    outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "    aeod_rand = outputs['fairness']\n",
    "    print(f'Absolute Equalized odds (rand): {aeod_rand:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(args, model, trainset, train_loader, sa_index, p_Group):\n",
    "    val_ids = np.random.choice(len(trainset), int(len(trainset) * 0.8), replace=False)\n",
    "    valset = Subset(trainset, val_ids)\n",
    "    val_loader = DataLoader(valset, batch_size=len(valset), shuffle=False, num_workers=0)\n",
    "\n",
    "    poisonids = [trainset[idx][2] for idx in range(len(trainset))]\n",
    "    poison_lookup = dict(zip(poisonids, range(len(trainset))))\n",
    "\n",
    "    poison_weights = weight_init(args, poisonids, trainset).to(args.device)\n",
    "    att_optimizer = torch.optim.Adam([poison_weights], lr=args.craftrate, weight_decay=0)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(att_optimizer, milestones=[args.ncraftstep // 2.667, args.ncraftstep // 1.6, args.ncraftstep // 1.142], gamma=0.1)\n",
    "    poison_weights.grad = torch.zeros_like(poison_weights)\n",
    "\n",
    "    loss_trace = []\n",
    "\n",
    "    for step in range(args.ncraftstep):\n",
    "\n",
    "        target_loss, n_batch = 0, 0\n",
    "\n",
    "        #optimizer_unlearned = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        optimizer_unlearned = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        model.train()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for batch, example in enumerate(train_loader):\n",
    "            inputs, targets, ids = example\n",
    "            inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "\n",
    "            poison_slices, batch_positions = [], []\n",
    "            for batch_id, sample_id in enumerate(ids.tolist()):\n",
    "                lookup = poison_lookup.get(sample_id)\n",
    "                if lookup is not None:\n",
    "                    poison_slices.append(lookup)\n",
    "                    batch_positions.append(batch_id)\n",
    "\n",
    "            if len(batch_positions) > 0:\n",
    "                weight_slice = poison_weights[poison_slices].detach().to(args.device)\n",
    "                weight_slice.requires_grad_()\n",
    "                h2 = 1 - torch.sigmoid(args.theta * (1 - 2 * weight_slice))\n",
    "\n",
    "            model.zero_grad()\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                with higher.innerloop_ctx(model, optimizer_unlearned) as (net, opt):\n",
    "                    result_z = net(inputs)\n",
    "                    loss_z = loss_func(result_z, targets)\n",
    "                    loss_mul = torch.mul(loss_z, h2).mean()\n",
    "                    opt.step(-loss_mul)\n",
    "\n",
    "                    net.eval()\n",
    "                    \"\"\"\n",
    "                    Here's loss function for fairness\n",
    "                    For performance degradation, we have loss function: \n",
    "                    ```\n",
    "                        Loss = criterion(val_outputs, val_labels)\n",
    "                    ```\n",
    "                    \n",
    "                    For targeted attack, we have loss function:\n",
    "                    ```\n",
    "                        Loss = criterion(target_outputs, target_label) + args.lam * criterion(val_outputs, val_labels)\n",
    "                    ``` \n",
    "                    \"\"\"\n",
    "                    for idx, (val_inputs, val_labels, _) in enumerate(val_loader):\n",
    "                        val_inputs, val_labels = val_inputs.to(args.device), val_labels.to(args.device)\n",
    "                        val_outputs = net(val_inputs)\n",
    "                        if args.floss == 'individual':\n",
    "                            fair_loss = -indiv_fair_loss(args, val_outputs, val_inputs, val_labels, sa_index, p_Group)\n",
    "                        elif args.floss == 'group':\n",
    "                            fair_loss = -group_fair_loss(args, val_outputs, val_inputs, val_labels, sa_index, p_Group)\n",
    "                        else:\n",
    "                            raise NotImplementedError('Not support!')\n",
    "\n",
    "                    fair_loss += args.lam * criterion(val_outputs, val_labels)\n",
    "\n",
    "                    grads = torch.autograd.grad(fair_loss, weight_slice)[0].detach()\n",
    "                    poison_weights.grad[poison_slices] = grads\n",
    "\n",
    "                    net.train()\n",
    "\n",
    "            target_loss += fair_loss.item()\n",
    "            n_batch += 1\n",
    "\n",
    "        if step % (args.ncraftstep // 5) == 0 or step == (args.ncraftstep - 1):\n",
    "            print(f'step: {step}, target loss: {target_loss / n_batch}')\n",
    "\n",
    "        loss_trace.append(target_loss / n_batch)\n",
    "\n",
    "        att_optimizer.step()\n",
    "        # scheduler.step()\n",
    "        att_optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            poison_weights.data = torch.clamp(poison_weights, 0, 1)\n",
    "\n",
    "    return loss_trace[0] - loss_trace[-1], poison_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, criterion, optimizer, train_loader, scheduler=None):\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        running_loss, n_batches, total, correct = 0.0, 0, 0, 0\n",
    "\n",
    "        for idx, (images, labels, _) in enumerate(train_loader):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        loss = running_loss / n_batches\n",
    "        accuracy = 100 * correct / total\n",
    "        print('Epoch %d training loss: %.3f training accuracy: %.2f%%' % (epoch, loss, accuracy))\n",
    "\n",
    "        #test(args, logging, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    predicts, probs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, labels, _) in enumerate(test_loader):\n",
    "            images, labels = images.to(args.device), labels.to(args.device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            predicts += predicted.detach().cpu().tolist()\n",
    "            probs += F.softmax(outputs, dim=1).detach().cpu().tolist()\n",
    "\n",
    "    print('Test accuracy: %.2f%%' % (100 * correct / total))\n",
    "\n",
    "    return predicts, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the victim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, sa_index, p_Group, protected_attribute, majority_group_name, minority_group_name, input_dim, output_dim = construct_dataset(args.dataset, args.datadir)\n",
    "\n",
    "trainset = PackData(X_train, y_train)\n",
    "testset = PackData(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "if args.model == 'logistic':\n",
    "    model = LogisticRegression(input_dim=input_dim, output_dim=output_dim).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    model = MLPClassifier(input_size=input_dim, hidden_sizes=[100], output_size=output_dim).to(args.device)\n",
    "elif args.model == 'mlp2':\n",
    "    model = MLPClassifier(input_size=input_dim, hidden_sizes=[100, 100], output_size=output_dim).to(args.device)\n",
    "else:\n",
    "    raise NotImplementedError('Not support!')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [int(args.epochs*0.5), int(args.epochs*0.75)], gamma=0.1)\n",
    "\n",
    "train(args, model, criterion, optimizer, train_loader)\n",
    "state = {\n",
    "    'net': model.state_dict(),\n",
    "    'epoch': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "}\n",
    "torch.save(state, os.path.join(args.moddir, args.dataset + '-' + args.model  + '.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the performance of the pretrain model\n",
    "y_predicts, y_pred_probs =test(args, model, test_loader)\n",
    "print('Absolute Equalized odds')\n",
    "outputs = calculate_performance_absolute_equalized_odds(X_test, y_test, y_predicts, y_pred_probs, sa_index, p_Group)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating unlearning request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights, scores = [], torch.ones(args.restarts) * 10_000\n",
    "for trial in range(args.restarts):\n",
    "    print(f'restart {trial}')\n",
    "\n",
    "    loss_diff, poison_weights = iteration(args, model, trainset, train_loader, sa_index, p_Group)\n",
    "    scores[trial] = loss_diff\n",
    "    weights.append(poison_weights.detach())\n",
    "\n",
    "optimal_score = torch.argmax(scores)\n",
    "stat_optimal_loss = scores[optimal_score].item()\n",
    "print(f'weights with maximum loss reduced {stat_optimal_loss:6.4e} selected.')\n",
    "poison_weights = weights[optimal_score]\n",
    "\n",
    "# save_results(args, poison_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the unlearning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victim(args, poison_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un_fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
